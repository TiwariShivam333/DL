{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assgn 1_DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XetxRf9xLJl1",
        "colab_type": "text"
      },
      "source": [
        "**1- User defined architecture of DNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeeXjQiAY634",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "e05db984-0e01-4f14-a3e7-a08b9a7e83f4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHU0Xn-BY2UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    return s\n",
        "\n",
        "def dsigmoid(x):\n",
        "  s=1/(1+np.exp(-x))\n",
        "  ds=s*(1-s)  \n",
        "  return ds\n",
        "\n",
        "def tanh(x):\n",
        "    t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    return t\n",
        "\n",
        "def dtanh(x):\n",
        "    t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    dt=1-t**2\n",
        "    return dt\n",
        "\n",
        "def relu(p):\n",
        "    return np.maximum(0, p)\n",
        "\n",
        "def drelu(x):\n",
        "    x[x <= 0] = 0\n",
        "    x[x > 0] = 1\n",
        "    return x\n",
        "\n",
        "leaky_slope=0.1\n",
        "def leakyrelu(x):\n",
        "  return np.maximum(leaky_slope*x,x)\n",
        "\n",
        "def dleakyrelu(x):\n",
        "  d=np.zeros_like(x)\n",
        "  d[x<=0]=leaky_slope\n",
        "  d[x>0]=1\n",
        "  return d\n",
        "  \n",
        "def softmax(u):\n",
        "    return np.exp(u) / np.sum(np.exp(u), axis=0, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V55xNuzZCtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing the Weights and Biases\n",
        "parameters = {}\n",
        "def initialize_parameters(layer_dims):\n",
        "    L = len(layer_dims)\n",
        "    for l in range(1, L):\n",
        "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * (np.sqrt(2 / layer_dims[l - 1]))\n",
        "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7J7dGrZspH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing the Cost\n",
        "def compute_cost(activation):\n",
        "    loss = - np.sum((Y_train * np.log(activation[\"A\"+str(n+1)])), axis=0, keepdims=True)\n",
        "    cost = np.sum(loss, axis=1) / m\n",
        "    return cost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJggdFGvaKmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions\n",
        "def predict(parameters, X_test):\n",
        "    forward_prop(parameters, X_test, activation)\n",
        "    predictions = np.round(activation[\"A\"+str(n+1)])\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGfN05AjVVzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ba469c7-e01e-4b32-992e-3617f8473379"
      },
      "source": [
        "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = mnist.load_data()\n",
        "\n",
        "Y_tr_resh = Y_train_orig.reshape(60000, 1)\n",
        "Y_te_resh = Y_test_orig.reshape(10000, 1)\n",
        "Y_tr_T = to_categorical(Y_tr_resh, num_classes=10)\n",
        "Y_te_T = to_categorical(Y_te_resh, num_classes=10)\n",
        "Y_train = Y_tr_T.T\n",
        "Y_test = Y_te_T.T\n",
        "\n",
        "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
        "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
        "\n",
        "X_train = X_train_flatten / 255.\n",
        "X_test = X_test_flatten / 255.\n",
        "print(Y_train.shape)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 60000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUnRORJUcqfI",
        "colab_type": "code",
        "outputId": "4629afea-1a3a-46e2-cd24-52d8d0a1e1c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "lst = [] \n",
        "lst.append(X_train.shape[0])\n",
        "m = X_train.shape[1]\n",
        "n = int(input(\"Enter number of Hidden layers : \")) \n",
        "print(\"Enter the size of hidden layers \")\n",
        "for i in range(0, n): \n",
        "    ele = int(input()) \n",
        "    lst.append(ele)\n",
        "\n",
        "lst.append(10)     \n",
        "print(lst) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of Hidden layers : 3\n",
            "Enter the size of hidden layers \n",
            "256\n",
            "128\n",
            "64\n",
            "[784, 256, 128, 64, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN279kCZjUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Forward Propagation\n",
        "outputs = {}\n",
        "activation = {}\n",
        "def forward_prop(parameters, X_train, activation, fn=0):\n",
        "    outputs[\"Z\" + str(1)] = np.dot(parameters[\"W1\"], X_train) + parameters[\"b1\"]\n",
        "    if fn==0:\n",
        "      activation[\"A\" + str(1)] = relu(outputs[\"Z\" + str(1)])\n",
        "    elif fn==1:\n",
        "      activation[\"A\" + str(1)] = sigmoid(outputs[\"Z\" + str(1)])\n",
        "    elif fn==2:\n",
        "      activation[\"A\" + str(1)] = tanh(outputs[\"Z\" + str(1)])\n",
        "    elif fn==3:\n",
        "      activation[\"A\" + str(1)] = leakyrelu(outputs[\"Z\" + str(1)])\n",
        "\n",
        "    for l in range(2, n+1):\n",
        "        outputs[\"Z\" + str(l)] = np.dot(parameters[\"W\" + str(l)], activation[\"A\" + str(l - 1)]) + parameters[\"b\" + str(l)]\n",
        "        if fn==0:\n",
        "          activation[\"A\" + str(l)] = relu(outputs[\"Z\" + str(l)])\n",
        "        elif fn==1:\n",
        "          activation[\"A\" + str(l)] = sigmoid(outputs[\"Z\" + str(l)])\n",
        "        elif fn==2:\n",
        "          activation[\"A\" + str(l)] = tanh(outputs[\"Z\" + str(l)])\n",
        "        elif fn==3:\n",
        "          activation[\"A\" + str(l)] = leakyrelu(outputs[\"Z\" + str(l)])\n",
        "        #activation[\"A\" + str(l)] = relu(outputs[\"Z\" + str(l)])\n",
        "    outputs[\"Z\"+str(n+1)] = np.dot(parameters[\"W\"+str(n+1)], activation[\"A\"+str(n)]) + parameters[\"b\"+str(n+1)]\n",
        "    activation[\"A\"+str(n+1)] = softmax(outputs[\"Z\"+str(n+1)])\n",
        "    return outputs, activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZDSjercaBZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grad_reg = {}\n",
        "m = X_train.shape[1]\n",
        "def grad_re(parameters, outputs, activation, fn=0):\n",
        "    grad_reg[\"dZ\"+str(n+1)] = (activation[\"A\"+str(n+1)] - Y_train) / m\n",
        "    for l in range(1, n+1):\n",
        "        grad_reg[\"dA\" + str(n+1 - l)] = np.dot(parameters[\"W\" + str(n+1 - l + 1)].T, grad_reg[\"dZ\" + str(n+1 - l + 1)])\n",
        "        if fn==0:\n",
        "          grad_reg[\"dZ\" + str(n+1 - l)] = grad_reg[\"dA\" + str(n+1 - l)] * drelu(outputs[\"Z\" + str(n+1 - l)])\n",
        "        elif fn==1:\n",
        "          grad_reg[\"dZ\" + str(n+1 - l)] = grad_reg[\"dA\" + str(n+1 - l)] * dsigmoid(outputs[\"Z\" + str(n+1 - l)])\n",
        "        elif fn==2:\n",
        "          grad_reg[\"dZ\" + str(n+1 - l)] = grad_reg[\"dA\" + str(n+1 - l)] * dtanh(outputs[\"Z\" + str(n+1 - l)])\n",
        "        elif fn==3:\n",
        "          grad_reg[\"dZ\" + str(n+1 - l)] = grad_reg[\"dA\" + str(n+1 - l)] * dleakyrelu(outputs[\"Z\" + str(n+1 - l)])\n",
        "    grad_reg[\"dW1\"] = np.dot(grad_reg[\"dZ1\"], X_train.T)\n",
        "    grad_reg[\"db1\"] = np.sum(grad_reg[\"dZ1\"], axis=1, keepdims=True)\n",
        "    for l in range(2, n+2):\n",
        "        grad_reg[\"dW\" + str(l)] = np.dot(grad_reg[\"dZ\" + str(l)], activation[\"A\" + str(l - 1)].T)\n",
        "        grad_reg[\"db\" + str(l)] = np.sum(grad_reg[\"dZ\" + str(l)], axis=1, keepdims=True)\n",
        "    return parameters, outputs, activation, grad_reg\n",
        "#grad_re(parameters, outputs, activation)\n",
        "def learning(grad_reg, learning_rate=0.005):\n",
        "    for i in range(1, n+2):\n",
        "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - (learning_rate * grad_reg[\"dW\" + str(i)])\n",
        "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - (learning_rate * grad_reg[\"db\" + str(i)])\n",
        "    return parameters\n",
        "#learning(parameters, grad_reg, learning_rate=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2dJ5672aVDL",
        "colab_type": "code",
        "outputId": "a44ad7a4-5c5e-4059-86f8-ba2d0ba4d2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Iterating over num_iterations\n",
        "print_cost = True\n",
        "costs = []\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def model(num_iterations, costs, activation, fn=0):\n",
        "    initialize_parameters(lst)\n",
        "    for l in range(0, num_iterations):\n",
        "        forward_prop(parameters, X_train, activation, fn)\n",
        "        cost = compute_cost(activation)\n",
        "        grad_re(parameters, outputs, activation, fn)\n",
        "        learning(grad_reg, learning_rate=0.02)\n",
        "        if l % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        if print_cost and l % 100 == 0:\n",
        "          forward_prop(parameters, X_test, activation, fn)\n",
        "          A=activation[\"A\"+str(n+1)]\n",
        "          predictions = np.argmax(A, axis=0)\n",
        "          labels = np.argmax(Y_test, axis=0)\n",
        "\n",
        "          #print(confusion_matrix(predictions, labels))\n",
        "          print(classification_report(predictions, labels))\n",
        "          print(\"Cost after iteration %i: %f\" % (l, cost))\n",
        "    return costs, parameters\n",
        "\n",
        "num_iterations = int(input(\"Enter number of iterations : \"))\n",
        "fn = int(input(\"Which activation function 0-relu, 1-sigmoid, 2-tanh, 3-leaky relu: \"))\n",
        "model(num_iterations, costs, activation, fn)\n",
        "forward_prop(parameters, X_test, activation, fn)\n",
        "A=activation[\"A\"+str(n+1)]\n",
        "predictions = np.argmax(A, axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "\n",
        "#print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of iterations : 600\n",
            "[[  0   0   1   0   1   0   0   5   0   1]\n",
            " [  6  12   7  13  11  34  20  63  35  15]\n",
            " [  9 178  23  99  28  38  18  36  14  27]\n",
            " [ 13  55  18  12  38  39  20  17  28  30]\n",
            " [ 17   0  44  15  45   6   2   4  25  27]\n",
            " [  0   0   0   0   0   0   0   0   0   0]\n",
            " [  2  10   4   9  11  11   1  50  14  13]\n",
            " [119 460 130  97 307 232  77 352 320 354]\n",
            " [  0   0   0   0   0   0   0   0   0   0]\n",
            " [814 420 805 765 541 532 820 501 538 542]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         8\n",
            "           1       0.01      0.06      0.02       216\n",
            "           2       0.02      0.05      0.03       470\n",
            "           3       0.01      0.04      0.02       270\n",
            "           4       0.05      0.24      0.08       185\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.00      0.01      0.00       125\n",
            "           7       0.34      0.14      0.20      2448\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.54      0.09      0.15      6278\n",
            "\n",
            "    accuracy                           0.10     10000\n",
            "   macro avg       0.10      0.06      0.05     10000\n",
            "weighted avg       0.42      0.10      0.15     10000\n",
            "\n",
            "Cost after iteration 0: 2.409962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 893    0   69    6    8   44  106    7   27   17]\n",
            " [   1 1084   34   11   24   54   37   29  100   24]\n",
            " [  13   18  676   45    3   21   28   31   30    2]\n",
            " [  21   11  157  890    3  461   60    9  423   16]\n",
            " [   1    0   19    2  699   37   30    9   28  169]\n",
            " [   0    0    1    0    0    8    0    0    0    0]\n",
            " [  29    4   17    8   13  105  668   12   14    7]\n",
            " [   2    1   13   10   47   80    3  676   31   94]\n",
            " [   4   17    6   11    2   29    6    7  204    3]\n",
            " [  16    0   40   27  183   53   20  248  117  677]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.76      0.83      1177\n",
            "           1       0.96      0.78      0.86      1398\n",
            "           2       0.66      0.78      0.71       867\n",
            "           3       0.88      0.43      0.58      2051\n",
            "           4       0.71      0.70      0.71       994\n",
            "           5       0.01      0.89      0.02         9\n",
            "           6       0.70      0.76      0.73       877\n",
            "           7       0.66      0.71      0.68       957\n",
            "           8       0.21      0.71      0.32       289\n",
            "           9       0.67      0.49      0.57      1381\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.64      0.70      0.60     10000\n",
            "weighted avg       0.77      0.65      0.69     10000\n",
            "\n",
            "Cost after iteration 100: 1.533574\n",
            "[[ 934    0   32    9    5   27   29    9   10   12]\n",
            " [   0 1083   20    6   19   24   14   39   44   20]\n",
            " [   4    3  812   34    4   15   15   26   13    3]\n",
            " [   3    3   50  830    0  206    3    0   87    5]\n",
            " [   0    0   18    1  876   25   14   10   18  107]\n",
            " [   4    2    4   30    0  371   14    0    8   11]\n",
            " [  26    4   29    6   15   92  861    5   20    5]\n",
            " [   0    1   15   11    9   39    1  841   17   63]\n",
            " [   8   39   37   66    6   61    7   19  703   15]\n",
            " [   1    0   15   17   48   32    0   79   54  768]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.88      0.91      1067\n",
            "           1       0.95      0.85      0.90      1269\n",
            "           2       0.79      0.87      0.83       929\n",
            "           3       0.82      0.70      0.76      1187\n",
            "           4       0.89      0.82      0.85      1069\n",
            "           5       0.42      0.84      0.56       444\n",
            "           6       0.90      0.81      0.85      1063\n",
            "           7       0.82      0.84      0.83       997\n",
            "           8       0.72      0.73      0.73       961\n",
            "           9       0.76      0.76      0.76      1014\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.80      0.81      0.80     10000\n",
            "weighted avg       0.83      0.81      0.81     10000\n",
            "\n",
            "Cost after iteration 200: 0.897154\n",
            "[[ 940    0   19    7    4   23   24    7   10   13]\n",
            " [   0 1090   14    3   10   15    6   34   21   13]\n",
            " [   3    2  852   31    4    7   11   33   11    1]\n",
            " [   0    3   36  819    0  103    0    0   60    5]\n",
            " [   0    0   15    0  895   20   17   11   15   72]\n",
            " [   6    2    7   62    0  571   16    2   18   14]\n",
            " [  21    4   25    5   15   59  879    2   17    0]\n",
            " [   1    1   16   12    2   28    0  876   16   47]\n",
            " [   9   33   38   57    8   41    5   11  773   18]\n",
            " [   0    0   10   14   44   25    0   52   33  826]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.90      0.93      1047\n",
            "           1       0.96      0.90      0.93      1206\n",
            "           2       0.83      0.89      0.86       955\n",
            "           3       0.81      0.80      0.80      1026\n",
            "           4       0.91      0.86      0.88      1045\n",
            "           5       0.64      0.82      0.72       698\n",
            "           6       0.92      0.86      0.89      1027\n",
            "           7       0.85      0.88      0.86       999\n",
            "           8       0.79      0.78      0.79       993\n",
            "           9       0.82      0.82      0.82      1004\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.86      0.85      0.85     10000\n",
            "\n",
            "Cost after iteration 300: 0.633694\n",
            "[[ 946    0   19    7    4   20   19    7   10   13]\n",
            " [   0 1101    9    3    7    7    4   26   12   10]\n",
            " [   1    3  867   27    4    8   10   35   10    3]\n",
            " [   1    3   30  841    0   68    0    0   52    5]\n",
            " [   0    0   17    0  897   17   16   11   13   56]\n",
            " [   7    1    7   60    0  649   16    3   20   18]\n",
            " [  16    4   22    4   15   45  889    1   16    0]\n",
            " [   1    1   20   12    1   23    0  894   12   35]\n",
            " [   8   22   34   41    7   37    4    9  802   13]\n",
            " [   0    0    7   15   47   18    0   42   27  856]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.93      1045\n",
            "           1       0.97      0.93      0.95      1179\n",
            "           2       0.84      0.90      0.87       968\n",
            "           3       0.83      0.84      0.84      1000\n",
            "           4       0.91      0.87      0.89      1027\n",
            "           5       0.73      0.83      0.78       781\n",
            "           6       0.93      0.88      0.90      1012\n",
            "           7       0.87      0.89      0.88       999\n",
            "           8       0.82      0.82      0.82       977\n",
            "           9       0.85      0.85      0.85      1012\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.88      0.87      0.88     10000\n",
            "\n",
            "Cost after iteration 400: 0.515611\n",
            "[[ 947    0   19    6    3   19   18    6    9   10]\n",
            " [   0 1102    6    2    7    7    3   23   10   10]\n",
            " [   1    3  874   27    4    9   10   36    9    4]\n",
            " [   1    3   29  859    0   48    0    1   39    6]\n",
            " [   0    0   17    0  901   17   15   10   13   50]\n",
            " [   7    0    7   56    0  687   15    3   24   17]\n",
            " [  15    4   21    4   14   36  893    1   14    0]\n",
            " [   1    2   20   14    1   18    0  910   10   28]\n",
            " [   8   21   33   29    8   37    4    5  823   10]\n",
            " [   0    0    6   13   44   14    0   33   23  874]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94      1037\n",
            "           1       0.97      0.94      0.96      1170\n",
            "           2       0.85      0.89      0.87       977\n",
            "           3       0.85      0.87      0.86       986\n",
            "           4       0.92      0.88      0.90      1023\n",
            "           5       0.77      0.84      0.80       816\n",
            "           6       0.93      0.89      0.91      1002\n",
            "           7       0.89      0.91      0.90      1004\n",
            "           8       0.84      0.84      0.84       978\n",
            "           9       0.87      0.87      0.87      1007\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.88     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Cost after iteration 500: 0.450014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5200cc31e428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter number of iterations : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Visualising the Cost vs num_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABfrY_EDobMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forward_prop(parameters, X_test, activation, fn)\n",
        "A=activation[\"A\"+str(n+1)]\n",
        "predictions = np.argmax(A, axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "\n",
        "#print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsauAGsj5Lh3",
        "colab_type": "text"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSEEEKogOwu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_augmentation(augment_size=5000): \n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "    \n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255.0\n",
        "    x_test /= 255.0\n",
        "    train_size = x_train.shape[0]\n",
        "    test_size = x_test.shape[0]\n",
        "    # Create one hot array\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test = to_categorical(y_test, 10)\n",
        "    image_generator = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        zoom_range = 0.05, \n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        horizontal_flip=False,\n",
        "        vertical_flip=False, \n",
        "        data_format=\"channels_last\",\n",
        "        zca_whitening=True)\n",
        "    # fit data for zca whitening\n",
        "    image_generator.fit(x_train, augment=True)\n",
        "    \n",
        "    randidx = np.random.randint(train_size, size=augment_size)\n",
        "    x_augmented = x_train[randidx].copy()\n",
        "    y_augmented = y_train[randidx].copy()\n",
        "    x_augmented = image_generator.flow(x_augmented, np.zeros(augment_size),\n",
        "                                batch_size=augment_size, shuffle=False).next()[0]\n",
        "    # append augmented data to trainset\n",
        "    x_train = np.concatenate((x_train, x_augmented))\n",
        "    y_train = np.concatenate((y_train, y_augmented))\n",
        "    train_size = x_train.shape[0]\n",
        "    test_size = x_test.shape[0]\n",
        "    Y_train = y_train.T\n",
        "    Y_test = y_test.T\n",
        "\n",
        "    X_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "    X_test = x_test.reshape(x_test.shape[0], -1).T\n",
        "    print(X_train.shape)\n",
        "    print(Y_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(Y_test.shape)\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU19aLbaPsvS",
        "colab_type": "code",
        "outputId": "dc22d9cf-228c-4efa-823b-c293b2440753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Iterating over num_iterations\n",
        "print_cost = True\n",
        "costs = []\n",
        "\n",
        "def model(num_iterations, costs, activation, fn=0):\n",
        "    initialize_parameters(lst)\n",
        "    for l in range(0, num_iterations):\n",
        "        forward_prop(parameters, X_train, activation)\n",
        "        cost = compute_cost(activation)\n",
        "        grad_re(parameters, outputs, activation)\n",
        "        learning(grad_reg, learning_rate=0.02)\n",
        "        if l % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        if print_cost and l % 100 == 0:\n",
        "          forward_prop(parameters, X_test, activation)\n",
        "          A=activation[\"A\"+str(n+1)]\n",
        "          predictions = np.argmax(A, axis=0)\n",
        "          labels = np.argmax(Y_test, axis=0)\n",
        "\n",
        "          #print(confusion_matrix(predictions, labels))\n",
        "          print(classification_report(predictions, labels))\n",
        "          '''\n",
        "          c=0 \n",
        "          y=Y_test.T\n",
        "          predictions = predict(parameters, X_test)\n",
        "          p=predictions.T\n",
        "          for i in range(10000):\n",
        "            if np.array_equal(p[i], y[i]):\n",
        "              c=c+1\n",
        "          print(c/10000)\n",
        "          print(\"Cost after iteration %i: %f\" % (l, cost))'''\n",
        "    return costs, parameters\n",
        "print(X_train.shape)\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "num_iterations = int(input(\"Enter number of iterations : \"))\n",
        "f = int(input(\"Data Augmentation 0 for No and 1 for Yes : \"))\n",
        "if f==1:\n",
        "  X_train, Y_train, X_test, Y_test= data_augmentation()\n",
        "  print(Y_train.shape)\n",
        "model(num_iterations, costs, activation)\n",
        "forward_prop(parameters, X_test, activation)\n",
        "A=activation[\"A\"+str(n+1)]\n",
        "predictions = np.argmax(A, axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "print(classification_report(predictions, labels))\n",
        "\n",
        "# Visualising the Cost vs num_iterations\n",
        "plt.plot(costs)\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cost')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 60000)\n",
            "(784, 60000)\n",
            "Enter number of iterations : 800\n",
            "Data Augmentation 0 for No and 1 for Yes : 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(784, 65000)\n",
            "(10, 65000)\n",
            "(784, 10000)\n",
            "(10, 10000)\n",
            "(10, 65000)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.07      0.03      0.04      2344\n",
            "           1       0.22      0.13      0.16      1937\n",
            "           2       0.01      0.55      0.01        11\n",
            "           3       0.00      0.25      0.01        12\n",
            "           4       0.05      0.10      0.07       463\n",
            "           5       0.02      0.03      0.02       446\n",
            "           6       0.12      0.08      0.10      1414\n",
            "           7       0.01      0.10      0.02        96\n",
            "           8       0.00      0.05      0.00        42\n",
            "           9       0.61      0.19      0.29      3235\n",
            "\n",
            "    accuracy                           0.11     10000\n",
            "   macro avg       0.11      0.15      0.07     10000\n",
            "weighted avg       0.28      0.11      0.15     10000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.87      0.91      1069\n",
            "           1       0.96      0.88      0.92      1244\n",
            "           2       0.78      0.83      0.80       970\n",
            "           3       0.83      0.79      0.81      1068\n",
            "           4       0.82      0.79      0.81      1025\n",
            "           5       0.62      0.86      0.72       646\n",
            "           6       0.90      0.87      0.88       993\n",
            "           7       0.85      0.87      0.86       996\n",
            "           8       0.75      0.77      0.76       958\n",
            "           9       0.78      0.76      0.77      1031\n",
            "\n",
            "    accuracy                           0.83     10000\n",
            "   macro avg       0.82      0.83      0.82     10000\n",
            "weighted avg       0.84      0.83      0.83     10000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94      1043\n",
            "           1       0.97      0.94      0.96      1175\n",
            "           2       0.83      0.87      0.85       988\n",
            "           3       0.86      0.85      0.86      1023\n",
            "           4       0.86      0.83      0.85      1017\n",
            "           5       0.75      0.88      0.81       767\n",
            "           6       0.91      0.90      0.90       972\n",
            "           7       0.88      0.90      0.89      1007\n",
            "           8       0.83      0.83      0.83       978\n",
            "           9       0.83      0.81      0.82      1030\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.88      0.87      0.87     10000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95      1034\n",
            "           1       0.97      0.95      0.96      1159\n",
            "           2       0.86      0.90      0.88       988\n",
            "           3       0.89      0.87      0.88      1022\n",
            "           4       0.90      0.87      0.88      1015\n",
            "           5       0.80      0.89      0.84       806\n",
            "           6       0.92      0.91      0.92       966\n",
            "           7       0.89      0.91      0.90      1008\n",
            "           8       0.85      0.85      0.85       977\n",
            "           9       0.87      0.85      0.86      1025\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.90      0.89      0.90     10000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.96      1032\n",
            "           1       0.98      0.96      0.97      1156\n",
            "           2       0.87      0.91      0.89       983\n",
            "           3       0.91      0.89      0.90      1034\n",
            "           4       0.91      0.89      0.90      1010\n",
            "           5       0.83      0.91      0.87       819\n",
            "           6       0.93      0.92      0.93       969\n",
            "           7       0.90      0.92      0.91      1008\n",
            "           8       0.87      0.87      0.87       974\n",
            "           9       0.88      0.88      0.88      1015\n",
            "\n",
            "    accuracy                           0.91     10000\n",
            "   macro avg       0.91      0.91      0.91     10000\n",
            "weighted avg       0.91      0.91      0.91     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AcOga0nxo33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9bdc5eaf-9ffc-464c-e8a8-7ec40b62e11e"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 65000)\n",
            "(65000,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}